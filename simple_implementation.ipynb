{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNY6E7eRlGbc0ZYs0doNkPJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/P-Karthikeya512/Movie-Recommendation-/blob/main/simple_implementation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "YBCvWhNfzexW"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch.autograd import Variable"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_data = [1, 2, 3]\n",
        "y_data = [2, 4, 6]"
      ],
      "metadata": {
        "id": "wdUeAutHzufv"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w1 = Variable(torch.tensor(1.0), requires_grad=True)\n",
        "w2 = Variable(torch.tensor(1.01), requires_grad=True)\n",
        "b = Variable(torch.tensor(1.0), requires_grad=True)"
      ],
      "metadata": {
        "id": "lHPjISsL5PFv"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def forward(x):\n",
        "  return w1 * (x ** 2) + w2 * x + b"
      ],
      "metadata": {
        "id": "LjIrmZVE5Vl-"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def loss(x, y):\n",
        "  y_pred = forward(x)\n",
        "  sq_error = (y_pred - y) ** 2\n",
        "  return sq_error"
      ],
      "metadata": {
        "id": "cg3OoPAZ5cqv"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"w1 before training is {w1} and w2 before training is {w2} and b before training is {b}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e0S258PiAY9n",
        "outputId": "e8bbc64b-5ce9-4093-9893-a1aa78ae5844"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "w1 before training is 1.0 and w2 before training is 1.0099999904632568 and b before training is 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.SGD([w1, w2, b], lr=0.001)\n",
        "\n",
        "for epoch in range(100):\n",
        "    for x, y in zip(x_data, y_data):\n",
        "\n",
        "        l = loss(x, y)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        l.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f\"progress: {epoch}, {l.item()}\")\n",
        "\n",
        "print(f\"w1 after training is {w1} and w2 before training is {w2} and b before training is {b}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a7_YdpKVAqc-",
        "outputId": "ede3f951-3d1d-4a21-ec4f-b5ce85db592f"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "progress: 0, 45.498199462890625\n",
            "progress: 1, 28.093631744384766\n",
            "progress: 2, 17.398597717285156\n",
            "progress: 3, 10.815547943115234\n",
            "progress: 4, 6.755023956298828\n",
            "progress: 5, 4.243813991546631\n",
            "progress: 6, 2.6856753826141357\n",
            "progress: 7, 1.7149631977081299\n",
            "progress: 8, 1.1071927547454834\n",
            "progress: 9, 0.724351704120636\n",
            "progress: 10, 0.48143649101257324\n",
            "progress: 11, 0.325970321893692\n",
            "progress: 12, 0.22546911239624023\n",
            "progress: 13, 0.15975259244441986\n",
            "progress: 14, 0.11622675508260727\n",
            "progress: 15, 0.0869949460029602\n",
            "progress: 16, 0.06707004457712173\n",
            "progress: 17, 0.05327949672937393\n",
            "progress: 18, 0.04358741641044617\n",
            "progress: 19, 0.036672867834568024\n",
            "progress: 20, 0.03166905418038368\n",
            "progress: 21, 0.02799909934401512\n",
            "progress: 22, 0.02527415193617344\n",
            "progress: 23, 0.02322838082909584\n",
            "progress: 24, 0.02167685702443123\n",
            "progress: 25, 0.020488740876317024\n",
            "progress: 26, 0.019571535289287567\n",
            "progress: 27, 0.018857186660170555\n",
            "progress: 28, 0.01829671487212181\n",
            "progress: 29, 0.017852718010544777\n",
            "progress: 30, 0.017497964203357697\n",
            "progress: 31, 0.017212145030498505\n",
            "progress: 32, 0.016978971660137177\n",
            "progress: 33, 0.016786780208349228\n",
            "progress: 34, 0.01662628911435604\n",
            "progress: 35, 0.01649031788110733\n",
            "progress: 36, 0.016373569145798683\n",
            "progress: 37, 0.01627183146774769\n",
            "progress: 38, 0.016181813552975655\n",
            "progress: 39, 0.016100876033306122\n",
            "progress: 40, 0.016027145087718964\n",
            "progress: 41, 0.0159588810056448\n",
            "progress: 42, 0.015895333141088486\n",
            "progress: 43, 0.015835152938961983\n",
            "progress: 44, 0.015777720138430595\n",
            "progress: 45, 0.015722665935754776\n",
            "progress: 46, 0.015669258311390877\n",
            "progress: 47, 0.015617371536791325\n",
            "progress: 48, 0.01556664239615202\n",
            "progress: 49, 0.015516826882958412\n",
            "progress: 50, 0.015467802993953228\n",
            "progress: 51, 0.015419448725879192\n",
            "progress: 52, 0.015371524728834629\n",
            "progress: 53, 0.015324265696108341\n",
            "progress: 54, 0.015277433209121227\n",
            "progress: 55, 0.015230907127261162\n",
            "progress: 56, 0.015184923075139523\n",
            "progress: 57, 0.015138890594244003\n",
            "progress: 58, 0.015093513764441013\n",
            "progress: 59, 0.015048205852508545\n",
            "progress: 60, 0.015003315173089504\n",
            "progress: 61, 0.01495849248021841\n",
            "progress: 62, 0.014913853257894516\n",
            "progress: 63, 0.0148696294054389\n",
            "progress: 64, 0.014825587160885334\n",
            "progress: 65, 0.014781611040234566\n",
            "progress: 66, 0.014737931080162525\n",
            "progress: 67, 0.014694430865347385\n",
            "progress: 68, 0.014651226811110973\n",
            "progress: 69, 0.014608316123485565\n",
            "progress: 70, 0.01456546876579523\n",
            "progress: 71, 0.01452268473803997\n",
            "progress: 72, 0.014480192214250565\n",
            "progress: 73, 0.014438106678426266\n",
            "progress: 74, 0.01439596712589264\n",
            "progress: 75, 0.01435423269867897\n",
            "progress: 76, 0.014312672428786755\n",
            "progress: 77, 0.014271172694861889\n",
            "progress: 78, 0.01422996073961258\n",
            "progress: 79, 0.014188808389008045\n",
            "progress: 80, 0.014147941954433918\n",
            "progress: 81, 0.014107247814536095\n",
            "progress: 82, 0.014066725969314575\n",
            "progress: 83, 0.014026261866092682\n",
            "progress: 84, 0.01398619543761015\n",
            "progress: 85, 0.013946184888482094\n",
            "progress: 86, 0.013906457461416721\n",
            "progress: 87, 0.013866674154996872\n",
            "progress: 88, 0.01382728386670351\n",
            "progress: 89, 0.013788062147796154\n",
            "progress: 90, 0.013749008066952229\n",
            "progress: 91, 0.01371012069284916\n",
            "progress: 92, 0.013671400025486946\n",
            "progress: 93, 0.013632734306156635\n",
            "progress: 94, 0.013594567775726318\n",
            "progress: 95, 0.013556343503296375\n",
            "progress: 96, 0.013518285006284714\n",
            "progress: 97, 0.01348038949072361\n",
            "progress: 98, 0.013442657887935638\n",
            "progress: 99, 0.013405201025307178\n",
            "w1 after training is 0.3114003837108612 and w2 before training is 0.7884982228279114 and b before training is 0.9266108274459839\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "forward(4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yBvBX0gTBdbP",
        "outputId": "9a7678a5-b770-4967-9ce5-910770f7ca9f"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(9.0630, grad_fn=<AddBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8gbZyL3TBhXP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}